@article{1stprize,
abstract = {In this technical report, we present our 1st place solution for the ICDAR 2021 competition on mathematical formula detection (MFD). The MFD task has three key challenges including a large scale span, large variation of the ratio between height and width, and rich character set and mathematical expressions. Considering these challenges, we used Generalized Focal Loss (GFL), an anchor-free method, instead of the anchor-based method, and prove the Adaptive Training Sampling Strategy (ATSS) and proper Feature Pyramid Network (FPN) can well solve the important issue of scale variation. Meanwhile, we also found some tricks, e.g., Deformable Convolution Network (DCN), SyncBN, and Weighted Box Fusion (WBF), were effective in MFD task. Our proposed method ranked 1st in the final 15 teams.},
archivePrefix = {arXiv},
arxivId = {2107.05534},
author = {Zhong, Yuxiang and Qi, Xianbiao and Li, Shanjun and Gu, Dengyi and Chen, Yihao and Ning, Peiyang and Xiao, Rong},
eprint = {2107.05534},
file = {:F\:/GitHub/HCMUS-Research-Methods/References/Aritcles/2107.05534.pdf:pdf},
pages = {1--8},
title = {{1st Place Solution for ICDAR 2021 Competition on Mathematical Formula Detection}},
url = {http://arxiv.org/abs/2107.05534},
year = {2021}
}

@article{f-rcnn,
abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features - using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3], our detection system has a frame rate of 5 fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
archivePrefix = {arXiv},
arxivId = {1506.01497},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
doi = {10.1109/TPAMI.2016.2577031},
eprint = {1506.01497},
file = {:F\:/Downloads/Documents/1506.01497.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Object detection,convolutional neural network,region proposal},
number = {6},
pages = {1137--1149},
pmid = {27295650},
title = {{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}},
volume = {39},
year = {2017}
}

@misc{resnet-50,
author = {Kaushik, Aakash},
title = {{Understanding ResNet50 architecture}},
url = {https://iq.opengenus.org/resnet50-architecture/}
}

@article{Chan2020,
abstract = {Offline handwritten mathematical expression recognition is often considered much harder than its online counterpart due to the absence of temporal information. In order to take advantage of the more mature methods for online recognition and save resources, an oversegmentation approach is proposed to recover strokes from textual bitmap images automatically. The proposed algorithm first breaks down the skeleton of a binarized image into junctions and segments, then segments are merged to form strokes, finally stroke order is normalized by using recursive projection and topological sort. Good offline accuracy was obtained in combination with ordinary online recognizers, which were not specially designed for extracted strokes. Given a ready-made state-of-the-art online handwritten mathematical expression recognizer, the proposed procedure correctly recognized $58.22\%$, $65.65\%$, and $65.22\%$ of the offline formulas rendered from the datasets of the Competitions on Recognition of Online Handwritten Mathematical Expressions (CROHME) in 2014, 2016, and 2019 respectively. Furthermore, given a trainable online recognition system, retraining it with extracted strokes resulted in an offline recognizer with the same level of accuracy.On the other hand, the speed of the entire pipeline was fast enough to facilitate on-device recognition on mobile phones with limited resources. To conclude, stroke extraction provides an attractive way to build optical character recognition software.  },
archivePrefix = {arXiv},
arxivId = {1905.06749},
author = {Chan, Chungkwong},
doi = {10.1109/ACCESS.2020.2984627},
eprint = {1905.06749},
file = {:F\:/GitHub/HCMUS-Research-Methods/References/Aritcles/1905.06749.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Character recognition,feature extraction,offline handwritten mathematical expression recogn,optical character recognition software,stroke extraction},
pages = {61565--61575},
title = {{Stroke Extraction for Offline Handwritten Mathematical Expression Recognition}},
volume = {8},
year = {2020}
}

@article{Wang2021,
abstract = {In this paper, we propose a deep neural network model with an encoder–decoder architecture that translates images of math formulas into their LaTeX markup sequences. The encoder is a convolutional neural network that transforms images into a group of feature maps. To better capture the spatial relationships of math symbols, the feature maps are augmented with 2D positional encoding before being unfolded into a vector. The decoder is a stacked bidirectional long short-term memory model integrated with the soft attention mechanism, which works as a language model to translate the encoder output into a sequence of LaTeX tokens. The neural network is trained in two steps. The first step is token-level training using the maximum likelihood estimation as the objective function. At completion of the token-level training, the sequence-level training objective function is employed to optimize the overall model based on the policy gradient algorithm from reinforcement learning. Our design also overcomes the exposure bias problem by closing the feedback loop in the decoder during sequence-level training, i.e., feeding in the predicted token instead of the ground truth token at every time step. The model is trained and evaluated on the IM2LATEX-100 K dataset and shows state-of-the-art performance on both sequence-based and image-based evaluation metrics.},
archivePrefix = {arXiv},
arxivId = {1908.11415},
author = {Wang, Zelun and Liu, Jyh Charn},
doi = {10.1007/s10032-020-00360-2},
eprint = {1908.11415},
file = {:F\:/GitHub/HCMUS-Research-Methods/References/Aritcles/s10032-020-00360-2.pdf:pdf},
issn = {14332825},
journal = {International Journal on Document Analysis and Recognition},
keywords = {Deep learning,Encoder–decoder,Image to \LaTeX,Math formulas,Reinforcement learning,Seq2seq model},
number = {1-2},
pages = {63--75},
publisher = {Springer Berlin Heidelberg},
title = {{Translating math formula images to \LaTeX sequences using deep neural networks with sequence-level training}},
volume = {24},
year = {2021}
}

@article{Zhang2018,
abstract = {Handwritten mathematical expression recognition is a challenging problem due to the complicated two-dimensional structures, ambiguous handwriting input and variant scales of handwritten math symbols. To settle this problem, recently we propose the attention based encoder-decoder model that recognizes mathematical expression images from two-dimensional layouts to one-dimensional LaTeX strings. In this study, we improve the encoder by employing densely connected convolutional networks as they can strengthen feature extraction and facilitate gradient propagation especially on a small training set. We also present a novel multi-scale attention model which is employed to deal with the recognition of math symbols in different scales and restore the fine-grained details dropped by pooling operations. Validated on the CROHME competition task, the proposed method significantly outperforms the state-of-the-art methods with an expression recognition accuracy of $52.8\%$ on CROHME 2014 and $50.1\%$ on CROHME 2016, by only using the official training dataset.},
archivePrefix = {arXiv},
arxivId = {1801.03530},
author = {Zhang, Jianshu and Du, Jun and Dai, Lirong},
doi = {10.1109/ICPR.2018.8546031},
eprint = {1801.03530},
file = {:F\:/GitHub/HCMUS-Research-Methods/References/Aritcles/1801.03530.pdf:pdf},
isbn = {9781538637883},
issn = {10514651},
journal = {Proceedings - International Conference on Pattern Recognition},
pages = {2245--2250},
title = {{Multi-Scale Attention with Dense Encoder for Handwritten Mathematical Expression Recognition}},
volume = {2018-Augus},
year = {2018}
}

@article{Mali2020,
abstract = {We introduce the Scanning Single Shot Detector (ScanSSD) for locating math formulas offset from text and embedded in textlines. ScanSSD uses only visual features for detection: no formatting or typesetting information such as layout, font, or character labels are employed. Given a 600 dpi document page image, a Single Shot Detector (SSD) locates formulas at multiple scales using sliding windows, after which candidate detections are pooled to obtain page-level results. For our experiments we use the TFD-ICDAR2019v2 dataset, a modification of the GTDB scanned math article collection. ScanSSD detects characters in formulas with high accuracy, obtaining a 0.926 f-score, and detects formulas with high recall overall. Detection errors are largely minor, such as splitting formulas at large whitespace gaps (e.g., for variable constraints) and merging formulas on adjacent textlines. Formula detection f-scores of 0.796 (IOU $\geq0.5$) and 0.733 (IOU $\ge 0.75$) are obtained. Our data, evaluation tools, and code are publicly available.},
archivePrefix = {arXiv},
arxivId = {2003.08005},
author = {Mali, Parag and Kukkadapu, Puneeth and Mahdavi, Mahshad and Zanibbi, Richard},
eprint = {2003.08005},
file = {:F\:/GitHub/HCMUS-Research-Methods/References/Aritcles/2003.08005.pdf:pdf},
title = {{ScanSSD: Scanning Single Shot Detector for Mathematical Formulas in PDF Document Images}},
url = {http://arxiv.org/abs/2003.08005},
year = {2020}
}

@article{Borus2017,
author = {Schester, Amit and Borus, Norah and Bakst, William},
file = {:F\:/GitHub/HCMUS-Research-Methods/References/Aritcles/5241761.pdf:pdf},
title = {{Converting Handwritten Mathematical Expressions into \LaTeX}},
year = {2017}
}

@article{Ma2021,
abstract = {Collaborative editing questions and answers plays an important role in quality control of Mathematics StackExchange which is a math Q&A Site. Our study of post edits in Mathematics Stack Exchange shows that there is a large number of math-related edits about latexifying formulas, revising LaTeX and converting the blurred math formula screenshots to LaTeX sequence. Despite its importance, manually editing one math-related post especially those with complex mathematical formulas is time-consuming and error-prone even for experienced users. To assist post owners and editors to do this editing, we have developed an edit-assistance tool, MathLatexEdit for formula latexification, LaTeX revision and screenshot transcription. We formulate this formula editing task as a translation problem, in which an original post is translated to a revised post. MathLatexEdit implements a deep learning based approach including two encoder-decoder models for textual and visual LaTeX edit recommendation with math-specific inference. The two models are trained on large-scale historical original-edited post pairs and synthesized screenshot-formula pairs. Our evaluation of MathLatexEdit not only demonstrates the accuracy of our model, but also the usefulness of MathLatexEdit in editing real-world posts which are accepted in Mathematics Stack Exchange.},
archivePrefix = {arXiv},
arxivId = {arXiv:2109.09343v1},
author = {Ma, Suyu and Chen, Chunyang and Khalajzadeh, Hourieh and Grundy, John},
doi = {10.1145/3479547},
eprint = {arXiv:2109.09343v1},
file = {:F\:/GitHub/HCMUS-Research-Methods/References/Aritcles/2109.09343.pdf:pdf},
issn = {25730142},
journal = {Proceedings of the ACM on Human-Computer Interaction},
keywords = {Q&A sites,collaborative editing,deep learning,latex,math},
number = {CSCW2},
publisher = {Association for Computing Machinery},
title = {{Latexify Math: Mathematical Formula Markup Revision to Assist Collaborative Editing in Math Q \& A Sites}},
volume = {5},
year = {2021}
}

@article{Chan2000,
abstract = {Automatic recognition of mathematical expressions is one of the key vehicles in the drive towards transcribing documents in scientific and engineering disciplines into electronic form. This problem typically consists of two major stages, namely, symbol recognition and structural analysis. In this survey paper, we will review most of the existing work with respect to each of the two major stages of the recognition process. In particular, we try to put emphasis on the similarities and differences between systems. Moreover, some important issues in mathematical expression recognition will be addressed in depth. All these together serve to provide a clear overall picture of how this research area has been developed to date. {\textcopyright} 2000 Springer-Verlag Berlin Heidelberg.},
author = {Chan, Kam Fai and Yeung, Dit Yan},
doi = {10.1007/PL00013549},
file = {:F\:/Downloads/Documents/10.1.1.29.3389.pdf:pdf},
issn = {14332833},
journal = {International Journal on Document Analysis and Recognition},
keywords = {Error detection and correction,Mathematical expression recognition,Performance evaluation,Structural analysis,Symbol recognition},
number = {1},
pages = {3--15},
title = {{Mathematical expression recognition: A survey}},
volume = {3},
year = {2000}
}

@article{Lin2011,
abstract = {Recognizing mathematical expressions in PDF documents is a new and important field in document analysis. It is quite different from extracting mathematical expressions in image-based documents. In this paper, we propose a novel method by combining rule-based and learning-based methods to detect both isolated and embedded mathematical expressions in PDF documents. Moreover, various features of formulas, including geometric layout, character and context content, are used to adapt to a wide range of formula types. Experimental results show satisfactory performance of the proposed method. Furthermore, the method has been successfully incorporated into a commercial software package for large-scale Chinese e-Book production. {\textcopyright} 2011 IEEE.},
author = {Lin, Xiaoyan and Gao, Liangcai and Tang, Zhi and Lin, Xiaofan and Hu, Xuan},
doi = {10.1109/ICDAR.2011.285},
file = {:F\:/GitHub/HCMUS-Research-Methods/References/Aritcles/lin2011.pdf:pdf},
isbn = {9780769545202},
issn = {15205363},
journal = {Proceedings of the International Conference on Document Analysis and Recognition, ICDAR},
keywords = {PDF document,formula extraction,mathematical expression recognition},
pages = {1419--1423},
title = {{Mathematical formula identification in PDF documents}},
year = {2011}
}

@inproceedings{Chaudhuri1998AnAF,
  title={An Approach for Processing Mathematical Expressions in Printed Document},
  author={Bidyut. B. Chaudhuri and Utpal Garain},
  booktitle={Document Analysis Systems},
  year={1998}
}

@article{Kacem2001,
author = {Kacem, Afef and Belaïd, Abdel and Ben Ahmed, Mohamed},
year = {2001},
month = {12},
pages = {97-108},
title = {Automatic Extraction of Printed Mathematical Formulas Using Fuzzy Logic and Propagation of Context},
volume = {4},
journal = {International Journal on Document Analysis and Recognition},
doi = {10.1007/s100320100064}
}

@inproceedings{Iwatsuki2017,
author = {Iwatsuki, Kenichi and Sagara, Takeshi and Hara, Tadayoshi and Aizawa, Akiko},
title = {Detecting In-Line Mathematical Expressions in Scientific Documents},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3103010.3121041},
doi = {10.1145/3103010.3121041},
pages = {141–144},
numpages = {4},
keywords = {in-line mathematical expression detection, math ir, mathematical formula recognition, scientific paper mining, pdf structure analysis},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{Gao2017,
author = {Gao, Liangcai and Yi, Xiaohan and Liao, Yuan and Jiang, Zhuoren and Yan, Zuoyu and Tang, Zhi},
year = {2017},
month = {11},
pages = {553-558},
title = {A Deep Learning-Based Formula Detection Method for PDF Documents},
doi = {10.1109/ICDAR.2017.96}
}

@article{Ohyama2019a,
abstract = {A detection method for mathematical expressions in scientific document images is proposed. Inspired by the promising performance of U-Net, a convolutional network architecture originally proposed for the semantic segmentation of biomedical images, the proposed method uses image conversion by a U-Net framework. The proposed method does not use any information from mathematical and linguistic grammar so that it can be a supplemental bypass in the conventional mathematical optical character recognition (OCR) process pipeline. The evaluation experiments confirmed that (1) the performance of mathematical symbol and expression detection by the proposed method is superior to that of InftyReader, which is state-of-the-art software for mathematical OCR; (2) the coverage of the training dataset to the variation of document style is important; and (3) retraining with small additional training samples will be effective to improve the performance. An additional contribution is the release of a dataset for benchmarking the OCR for scientific documents.},
author = {Ohyama, Wataru and Suzuki, Masakazu and Uchida, Seiichi},
doi = {10.1109/ACCESS.2019.2945825},
file = {:F\:/GitHub/HCMUS-Research-Methods/References/Aritcles/ohyama2019.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Character recognition,neural networks,object detection},
pages = {144030--144042},
publisher = {IEEE},
title = {{Detecting Mathematical Expressions in Scientific Document Images Using a U-Net Trained on a Diverse Dataset}},
volume = {7},
year = {2019}
}

@article{Garain2007,
abstract = {Trypanosoma cruzi infection, transmitted by insect vectors or blood transfusions, is an important cause of morbidity and mortality in many Latin American countries. Treatments are toxic and frequently ineffective in curing patients with chronic manifestations of the infection (Chagas disease). Potentially exploitable chemotherapeutic targets of T. cruzi are enzymes of the sterol biosynthesis pathway. In particular, the P 450 enzyme, lanosterol 14$\alpha$-demethylase, has been implicated as the target of azole antifungal drugs that have potent anti-T. cruzi activity. In the work reported here, the T. cruzi lanosterol 14$\alpha$-demethylase (Tc14DM) gene was cloned by degenerate PCR. The gene was found to be expressed in both insect and mammalian life-cycle stages of the parasite. Tc14DM was able to complement the function of the homologous gene in yeast (erg11) as demonstrated by restored ergosterol prodn. in an erg11-deficient yeast strain. When the yeast strain was co-transfected with the P 450 reductase gene from Trypanosoma brucei, the amt. of ergosterol prodn. was increased, indicating that the endogenous yeast P 450 reductase was an inefficient partner with Tc14DM. Heterologous expression of Tc14DM in the baculovirus/Sf9 system resulted in a 52 kDa product. The protein was obsd. to have the characteristic absorbance spectra of a P 450 enzyme. A typical Type II binding spectrum was produced when the imidazole compd., ketoconazole, was mixed with the Tc14DM, demonstrating that ketoconazole binds the enzyme. [on SciFinder(R)]},
author = {Garain, Utpal and Chaudhuri, Bidyut B.},
doi = {10.1007/978-1-84628-726-8_11},
file = {:F\:/GitHub/HCMUS-Research-Methods/References/Aritcles/garain2007.pdf:pdf},
pages = {235--259},
title = {{OCR of Printed Mathematical Expressions}},
year = {2007}
}

@article{Liu2017,
abstract = {In this paper, we study the problem of math formula recognition (MFR) in degraded Chinese document images. Compared to traditional optical character recognition (OCR), the MFR problem brings new challenges in terms of character segmentation and structural analysis, especially in degraded images. To tackle these issues, we propose an over-segmentation strategy to split and recognize adhesive formula elements based on convolutional neural network (CNN). In addition, we propose a hierarchical framework for formula structure analysis that constructs the formula in a top-down manner to iteratively split the regions into recognizable units. Due to the lack of degraded Chinese document images with math formulas in the community, we also harvest a diverse ground-Truth dataset containing 100 images submitted from our system users. Extended experiments demonstrate the effectiveness and robustness of our proposed method in comparison with state-of-The-art methods.},
author = {Liu, Ning and Zhang, Dongxiang and Xu, Xing and Guo, Long and Chen, Lijiang and Liu, Wenju and Ke, Dengfeng},
doi = {10.1109/ICDAR.2017.27},
file = {:F\:/GitHub/HCMUS-Research-Methods/References/Aritcles/ICDAR.2017.27.pdf:pdf},
isbn = {9781538635865},
issn = {15205363},
journal = {Proceedings of the International Conference on Document Analysis and Recognition, ICDAR},
keywords = {Chinese Document Image,Convolutional Neural Network,Math Formula Recognition},
pages = {113--118},
title = {{Robust Math Formula Recognition in Degraded Chinese Document Images}},
volume = {1},
year = {2017}
}

@article{Mahdavi2019,
abstract = {We summarize the tasks, protocol, and outcome for the 6th Competition on Recognition of Handwritten Mathematical Expressions (CROHME), which includes a new formula detection in document images task (+ TFD). For CROHME + TFD 2019, participants chose between two tasks for recognizing handwritten formulas from 1) online stroke data, or 2) images generated from the handwritten strokes. To compare LATEX strings and the labeled directed trees over strokes (label graphs) used in previous CROHMEs, we convert LATEX and stroke-based label graphs to label graphs defined over symbols (symbol-level label graphs, or symLG). More than thirty (33) participants registered for the competition, with nineteen (19) teams submitting results. The strongest formula recognition results were produced by the USTC-iFLYTEK research team, for both stroke-based ($81\%$) and image-based ($77\%$) input. For the new typeset formula detection task, the Samsung R&D Institute Ukraine (Team 2) obtained a very strong F-score ($93\%$). System performance has improved since the last CROHME-still, the competition results suggest that recognition of handwritten formulae remains a difficult structural pattern recognition task.},
author = {Mahdavi, Mahshad and Zanibbi, Richard and Mouchere, Harold and Viard-Gaudin, Christian and Garain, Utpal},
doi = {10.1109/ICDAR.2019.00247},
file = {:F\:/GitHub/HCMUS-Research-Methods/References/Aritcles/mahdavi2019.pdf:pdf},
isbn = {9781728128610},
issn = {15205363},
journal = {Proceedings of the International Conference on Document Analysis and Recognition, ICDAR},
keywords = {Formula detection,Handwriting recognition,Mathematical expression recognition,Performance evaluation},
pages = {1533--1538},
title = {{ICDAR 2019 CROHME + TFD: Competition on recognition of handwritten mathematical expressions and typeset formula detection}},
year = {2019}
}

@article{Phong2020,
author = {Phong, Bui Hai and Hoang, Thang Manh and Le, Thi Lan},
doi = {10.1109/ACCESS.2020.2992067},
file = {:F\:/GitHub/HCMUS-Research-Methods/References/Aritcles/phong2020.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Mathematical expression detection,document analysis,fusion technique,machine learning,neural network},
pages = {83663--83684},
title = {{A Hybrid Method for Mathematical Expression Detection in Scientific Document Images}},
volume = {8},
year = {2020}
}